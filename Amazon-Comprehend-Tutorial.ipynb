{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fd3851",
   "metadata": {},
   "source": [
    "# Amazon Comprehend API Tutorial\n",
    "## Step 1 - Setting Up\n",
    "This is a tutorial of using Amazon Comprehend API. This is an API designed for generating insights for business products, but you can always try different features of it by accessing its console or accessing through its APIs. This is suitable for you if you are interested in applications of machine learning/natural language processing, even if you are not a business owner. \n",
    "\n",
    "The first thing is to make sure you have an AWS account and a user that has access to Comprehend services. Then you will need to set up AWS CLI(Command Line Interface). In this blog, we will use AWS SageMaker. \n",
    "\n",
    "To begin using AWS SageMaker, we will need to create a notebook instance. After that, we could open the notebook in Jupyter and start by creating a new python-conda3 file. In this case, since we are using AWS SageMaker, the AWS CLI should come in pre-installed. To check this, run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544a8d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3()                                                                      S3()\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1mNAME\u001b[0m\r\n",
      "       s3 -\r\n",
      "\r\n",
      "\u001b[1mDESCRIPTION\u001b[0m\r\n",
      "       This  section  explains  prominent concepts and notations in the set of\r\n",
      "       high-level S3 commands provided.\r\n",
      "\r\n",
      "       If you are looking for the low level S3 commands for  the  CLI,  please\r\n",
      "       see the \u001b[1ms3api \u001b[22mcommand \u001b[4mreference\u001b[24m \u001b[4mpage\u001b[24m.\r\n",
      "\r\n",
      "   \u001b[1mPath Argument Type\u001b[0m\r\n",
      "       Whenever using a command, at least one path argument must be specified.\r\n",
      "       There are two types of path arguments: \u001b[1mLocalPath \u001b[22mand \u001b[1mS3Uri\u001b[22m.\r\n",
      "\r\n",
      "       \u001b[1mLocalPath\u001b[22m: represents the path of a local file or directory.  It can be\r\n",
      "       written as an absolute path or relative path.\r\n",
      "\r\n",
      "       \u001b[1mS3Uri\u001b[22m: represents the location of a S3 object, prefix, or bucket.  This\r\n",
      "       must be written in the form \u001b[1ms3://mybucket/mykey \u001b[22mwhere \u001b[1mmybucket  \u001b[22mis  the\r\n",
      "       specified  S3 bucket, \u001b[1mmykey \u001b[22mis the specified S3 key.  The path argument\r\n",
      "       must begin with \u001b[1ms3:// \u001b[22min order to denote that the path argument  refers\r\n",
      "       to  a  S3  object. Note that prefixes are separated by forward slashes.\r\n",
      "       For example, if the S3 object \u001b[1mmyobject \u001b[22mhad the prefix \u001b[1mmyprefix\u001b[22m, the  S3\r\n",
      "       key  would  be  \u001b[1mmyprefix/myobject\u001b[22m,  and if the object was in the bucket\r\n",
      "       \u001b[1mmybucket\u001b[22m, the \u001b[1mS3Uri \u001b[22mwould be \u001b[1ms3://mybucket/myprefix/myobject\u001b[22m.\r\n",
      "\r\n",
      "       \u001b[1mS3Uri \u001b[22malso supports S3 access points. To specify an access point,  this\r\n",
      "       value must be of the form \u001b[1ms3://<access-point-arn>/<key>\u001b[22m. For example if\r\n",
      "       the  access   point   \u001b[1mmyaccesspoint   \u001b[22mto   be   used   has   the   ARN:\r\n",
      "       \u001b[1marn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint   \u001b[22mand   the\r\n",
      "       object being accessed has the key \u001b[1mmykey\u001b[22m, then the \u001b[1mS3URI \u001b[22mused  must  be:\r\n",
      "       \u001b[1ms3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykey\u001b[22m.\r\n",
      "       Similar to bucket names, you can also use prefixes  with  access  point\r\n",
      "       ARNs         for         the         \u001b[1mS3Uri\u001b[22m.         For        example:\r\n",
      "       \u001b[1ms3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mypre-\u001b[0m\r\n",
      "       \u001b[1mfix/\u001b[0m\r\n",
      "\r\n",
      "       The  higher  level \u001b[1ms3 \u001b[22mcommands do \u001b[1mnot \u001b[22msupport access point object ARNs.\r\n",
      "       For     example,     if     the      following      was      specified:\r\n",
      "       \u001b[1ms3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccess-\u001b[0m\r\n",
      "       \u001b[1mpoint/object/mykey  \u001b[22mthe  \u001b[1mS3URI  \u001b[22mwill  resolve   to   the   object   key\r\n",
      "       \u001b[1mobject/mykey\u001b[0m\r\n",
      "\r\n",
      "   \u001b[1mOrder of Path Arguments\u001b[0m\r\n",
      "       Every  command  takes  one or two positional path arguments.  The first\r\n",
      "       path argument represents the source, which is the local  file/directory\r\n",
      "       or  S3  object/prefix/bucket  that  is being referenced.  If there is a\r\n",
      "       second path argument, it represents the destination, which is the local\r\n",
      "       file/directory  or  S3  object/prefix/bucket that is being operated on.\r\n",
      "       Commands with only one path argument do not have a destination  because\r\n",
      "       the operation is being performed only on the source.\r\n",
      "\r\n",
      "   \u001b[1mSingle Local File and S3 Object Operations\u001b[0m\r\n",
      "       Some  commands  perform operations only on single files and S3 objects.\r\n",
      "       The following commands are single file/object operations if no \u001b[1m--recur-\u001b[0m\r\n",
      "       \u001b[1msive \u001b[22mflag is provided.\r\n",
      "\r\n",
      "          +\bo \u001b[1mcp\u001b[0m\r\n",
      "\r\n",
      "          +\bo \u001b[1mmv\u001b[0m\r\n",
      "\r\n",
      "          +\bo \u001b[1mrm\u001b[0m\r\n",
      "\r\n",
      "       For  this  type of operation, the first path argument, the source, must\r\n",
      "       exist and be a local file or S3 object.  The second path argument,  the\r\n",
      "       destination,  can  be  the  name  of  a local file, local directory, S3\r\n",
      "       object, S3 prefix, or S3 bucket.\r\n",
      "\r\n",
      "       The destination is indicated as a local directory,  S3  prefix,  or  S3\r\n",
      "       bucket if it ends with a forward slash or back slash.  The use of slash\r\n",
      "       depends on the path argument type.  If the path argument  is  a  \u001b[1mLocal-\u001b[0m\r\n",
      "       \u001b[1mPath\u001b[22m,  the type of slash is the separator used by the operating system.\r\n",
      "       If the path is a \u001b[1mS3Uri\u001b[22m, the forward slash must always be  used.   If  a\r\n",
      "       slash  is at the end of the destination, the destination file or object\r\n",
      "       will adopt the name of the source file or object.  Otherwise, if  there\r\n",
      "       is no slash at the end, the file or object will be saved under the name\r\n",
      "       provided.  See examples in \u001b[1mcp \u001b[22mand \u001b[1mmv \u001b[22mto illustrate this description.\r\n",
      "\r\n",
      "   \u001b[1mDirectory and S3 Prefix Operations\u001b[0m\r\n",
      "       Some commands only perform operations on the contents of a local direc-\r\n",
      "       tory  or  S3 prefix/bucket.  Adding or omitting a forward slash or back\r\n",
      "       slash to the end of any path argument, depending on its type, does  not\r\n",
      "       affect  the  results  of  the  operation.   The following commands will\r\n",
      "       always result in a directory or S3 prefix/bucket operation:\r\n",
      "\r\n",
      "       +\bo \u001b[1msync\u001b[0m\r\n",
      "\r\n",
      "       +\bo \u001b[1mmb\u001b[0m\r\n",
      "\r\n",
      "       +\bo \u001b[1mrb\u001b[0m\r\n",
      "\r\n",
      "       +\bo \u001b[1mls\u001b[0m\r\n",
      "\r\n",
      "   \u001b[1mUse of Exclude and Include Filters\u001b[0m\r\n",
      "       Currently, there is no support for the use of UNIX style wildcards in a\r\n",
      "       command's  path  arguments.   However,  most  commands  have  \u001b[1m--exclude\u001b[0m\r\n",
      "       \u001b[1m\"<value>\" \u001b[22mand \u001b[1m--include  \"<value>\"  \u001b[22mparameters  that  can  achieve  the\r\n",
      "       desired  result.   These  parameters perform pattern matching to either\r\n",
      "       exclude or include a particular file or object.  The following  pattern\r\n",
      "       symbols are supported.\r\n",
      "\r\n",
      "          +\bo \u001b[1m*\u001b[22m: Matches everything\r\n",
      "\r\n",
      "          +\bo \u001b[1m?\u001b[22m: Matches any single character\r\n",
      "\r\n",
      "          +\bo \u001b[1m[sequence]\u001b[22m: Matches any character in \u001b[1msequence\u001b[0m\r\n",
      "\r\n",
      "          +\bo \u001b[1m[!sequence]\u001b[22m: Matches any character not in \u001b[1msequence\u001b[0m\r\n",
      "\r\n",
      "       Any  number of these parameters can be passed to a command.  You can do\r\n",
      "       this by providing an \u001b[1m--exclude \u001b[22mor \u001b[1m--include  \u001b[22margument  multiple  times,\r\n",
      "       e.g.   \u001b[1m--include  \"*.txt\"  --include  \"*.png\"\u001b[22m.  When there are multiple\r\n",
      "       filters, the rule is the filters that appear later in the command  take\r\n",
      "       precedence  over filters that appear earlier in the command.  For exam-\r\n",
      "       ple, if the filter parameters passed to the command were\r\n",
      "\r\n",
      "          --exclude \"*\" --include \"*.txt\"\r\n",
      "\r\n",
      "       All files will be excluded from the command  except  for  files  ending\r\n",
      "       with  \u001b[1m.txt   \u001b[22mHowever, if the order of the filter parameters was changed\r\n",
      "       to\r\n",
      "\r\n",
      "          --include \"*.txt\" --exclude \"*\"\r\n",
      "\r\n",
      "       All files will be excluded from the command.\r\n",
      "\r\n",
      "       Each filter is evaluated against the \u001b[1msource directory\u001b[22m.  If  the  source\r\n",
      "       location is a file instead of a directory, the directory containing the\r\n",
      "       file is used as the source directory.  For example, suppose you had the\r\n",
      "       following directory structure:\r\n",
      "\r\n",
      "          /tmp/foo/\r\n",
      "            .git/\r\n",
      "            |---config\r\n",
      "            |---description\r\n",
      "            foo.txt\r\n",
      "            bar.txt\r\n",
      "            baz.jpg\r\n",
      "\r\n",
      "       In  the  command \u001b[1maws s3 sync /tmp/foo s3://bucket/ \u001b[22mthe source directory\r\n",
      "       is \u001b[1m/tmp/foo\u001b[22m.  Any include/exclude filters will be  evaluated  with  the\r\n",
      "       source  directory prepended.  Below are several examples to demonstrate\r\n",
      "       this.\r\n",
      "\r\n",
      "       Given the directory structure above and the command \u001b[1maws s3 cp  /tmp/foo\u001b[0m\r\n",
      "       \u001b[1ms3://bucket/  --recursive --exclude \".git/*\"\u001b[22m, the files \u001b[1m.git/config \u001b[22mand\r\n",
      "       \u001b[1m.git/description \u001b[22mwill be excluded from the files to upload because  the\r\n",
      "       exclude  filter  \u001b[1m.git/*  \u001b[22mwill  have the source prepended to the filter.\r\n",
      "       This means that:\r\n",
      "\r\n",
      "          /tmp/foo/.git/* -> /tmp/foo/.git/config       (matches, should exclude)\r\n",
      "          /tmp/foo/.git/* -> /tmp/foo/.git/description  (matches, should exclude)\r\n",
      "          /tmp/foo/.git/* -> /tmp/foo/foo.txt  (does not match, should include)\r\n",
      "          /tmp/foo/.git/* -> /tmp/foo/bar.txt  (does not match, should include)\r\n",
      "          /tmp/foo/.git/* -> /tmp/foo/baz.jpg  (does not match, should include)\r\n",
      "\r\n",
      "       The command \u001b[1maws s3  cp  /tmp/foo/  s3://bucket/  --recursive  --exclude\u001b[0m\r\n",
      "       \u001b[1m\"ba*\" \u001b[22mwill exclude \u001b[1m/tmp/foo/bar.txt \u001b[22mand \u001b[1m/tmp/foo/baz.jpg\u001b[22m:\r\n",
      "\r\n",
      "          /tmp/foo/ba* -> /tmp/foo/.git/config      (does not match, should include)\r\n",
      "          /tmp/foo/ba* -> /tmp/foo/.git/description (does not match, should include)\r\n",
      "          /tmp/foo/ba* -> /tmp/foo/foo.txt          (does not match, should include)\r\n",
      "          /tmp/foo/ba* -> /tmp/foo/bar.txt  (matches, should exclude)\r\n",
      "          /tmp/foo/ba* -> /tmp/foo/baz.jpg  (matches, should exclude)\r\n",
      "\r\n",
      "       Note that, by default, \u001b[4mall\u001b[24m \u001b[4mfiles\u001b[24m \u001b[4mare\u001b[24m \u001b[4mincluded\u001b[24m.  This means that provid-\r\n",
      "       ing \u001b[1monly \u001b[22man \u001b[1m--include \u001b[22mfilter will not  change  what  files  are  trans-\r\n",
      "       ferred.   \u001b[1m--include  \u001b[22mwill only re-include files that have been excluded\r\n",
      "       from an \u001b[1m--exclude \u001b[22mfilter.  If you only want to upload files with a par-\r\n",
      "       ticular extension, you need to first exclude all files, then re-include\r\n",
      "       the files with the particular extension.  This command will upload \u001b[1monly\u001b[0m\r\n",
      "       files ending with \u001b[1m.jpg\u001b[22m:\r\n",
      "\r\n",
      "          aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \"*\" --include \"*.jpg\"\r\n",
      "\r\n",
      "       If  you wanted to include both \u001b[1m.jpg \u001b[22mfiles as well as \u001b[1m.txt \u001b[22mfiles you can\r\n",
      "       run:\r\n",
      "\r\n",
      "          aws s3 cp /tmp/foo/ s3://bucket/ --recursive \\\r\n",
      "              --exclude \"*\" --include \"*.jpg\" --include \"*.txt\"\r\n",
      "\r\n",
      "\u001b[1mSYNOPSIS\u001b[0m\r\n",
      "          aws s3 <Command> [<Arg> ...]\r\n",
      "\r\n",
      "\u001b[1mOPTIONS\u001b[0m\r\n",
      "       \u001b[4mNone\u001b[0m\r\n",
      "\r\n",
      "\u001b[1mAVAILABLE COMMANDS\u001b[0m\r\n",
      "       +\bo cp\r\n",
      "\r\n",
      "       +\bo ls\r\n",
      "\r\n",
      "       +\bo mb\r\n",
      "\r\n",
      "       +\bo mv\r\n",
      "\r\n",
      "       +\bo presign\r\n",
      "\r\n",
      "       +\bo rb\r\n",
      "\r\n",
      "       +\bo rm\r\n",
      "\r\n",
      "       +\bo sync\r\n",
      "\r\n",
      "       +\bo website\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                          S3()\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e54868",
   "metadata": {},
   "source": [
    "## Step 2 - Checking AWS S3 Buckets\n",
    "The next step is to interact with S3 buckets using AWS CLI. The introduction to S3 buckets can be found in this [link](https://aws.amazon.com/s3/). Run the following code to list the S3 buckets you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230d9294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-28 23:19:03 myaswabcdaksldjdoidasjdk\r\n",
      "2023-03-26 22:04:01 myawsbucket-groupproject\r\n",
      "2023-02-22 18:14:52 myawsbucketfordaylight\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e38526",
   "metadata": {},
   "source": [
    "Next, you will need to upload your .txt test files to the S3 bucket being utilized, in this case, I used myawsbucket-groupproject. A recommended method for uploading files to an S3 bucket involves navigating to the S3 console, selecting the desired bucket, and choosing the \"upload\" option. From there, you can select the files you wish to upload and proceed to transfer them to the specified S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268720a0",
   "metadata": {},
   "source": [
    "## Step 3 - Using AWS Comprehend to Get Insights from the Text\n",
    "AWS Comprehend allows users to use both synchronous and asynchronous analysis of the text. Here, mainly synchronous analysis for the purpose of demonstration. However, asynchronous analysis of named entities will also be illustrated to show the interaction between AWS Comprehend and S3 buckets. AWS Comprehend provides with different NLP services to analyze the text, and here three different services are included: Named Entity Recognition, Sentiment Analysis, and Targeted Sentiment Analysis. \n",
    "\n",
    "### Named Entity Recognition\n",
    "This is the NLP service from AWS Comprehend to detect named entities - a real world object like a location - within a sentence or sentences. A list of all possible entities from AWS Comprehend is included below. <img src = 'https://drive.google.com/uc?export=view&id=15OUwjtNiVwpjJ_zWMl3vu2DCknJ7xTox' width = 550>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066fa00",
   "metadata": {},
   "source": [
    "#### Synchronous Method\n",
    "To obtain the Named Entity from textual data synchronously, execute the code snippet below. The required input parameters comprise the region of operation,  the language code of the text to be analyzed, and the target text itself. As an example, I chose to analyze \"The weather in Atlanta is nice today\". Comprehend returned two named entities: \"Atlanta\" as a location and \"today\" as a date in a json query. It also provides the \"BeginOffset\" and \"EndOffset\" of the recognized entities. There is also a score assigned to the entity, which is a confidence score that the service has in the accuracy of the entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b2f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"Entities\": [\r\n",
      "        {\r\n",
      "            \"Score\": 0.9985149502754211,\r\n",
      "            \"Type\": \"LOCATION\",\r\n",
      "            \"Text\": \"Atlanta\",\r\n",
      "            \"BeginOffset\": 15,\r\n",
      "            \"EndOffset\": 22\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"Score\": 0.9952268600463867,\r\n",
      "            \"Type\": \"DATE\",\r\n",
      "            \"Text\": \"today\",\r\n",
      "            \"BeginOffset\": 31,\r\n",
      "            \"EndOffset\": 36\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws comprehend detect-entities \\\n",
    "    --region us-east-1 \\\n",
    "    --language-code \"en\" \\\n",
    "    --text \"The weather in Atlanta is nice today\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159b64f",
   "metadata": {},
   "source": [
    "#### Asynchronous Method\n",
    "To obtain the Named Entity from textual data asynchronously, execute the code snippet below. The necessary input parameters include the S3 URL of the input text, the output destination, the ARN of the AWS role with access to the data and AWS Comprehend, a user-defined name for the asynchronous analysis job, the region of operation, and the language code of the text to be analyzed.\n",
    "\n",
    "If this request to start the job is sucessful, the following response will be received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81346ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"JobId\": \"1419b6498a3de8563a8ef6a31cdcc327\",\r\n",
      "    \"JobArn\": \"arn:aws:comprehend:us-east-1:139228718159:entities-detection-job/1419b6498a3de8563a8ef6a31cdcc327\",\r\n",
      "    \"JobStatus\": \"SUBMITTED\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws comprehend start-entities-detection-job \\\n",
    "--input-data-config S3Uri=s3://myawsbucket-groupproject/TestText/Entity_Recog_test.txt,InputFormat=ONE_DOC_PER_LINE \\\n",
    "--output-data-config S3Uri=s3://myawsbucket-groupproject/ \\\n",
    "--data-access-role-arn arn:aws:iam::139228718159:role/LabRole \\\n",
    "--job-name entity_detection_test\\\n",
    "--region us-east-1\\\n",
    "--language-code 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c64f9",
   "metadata": {},
   "source": [
    "You can check the status of the job through the following code. You can specify the region of the job and filter based on the job name you defined in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ba7edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"EntitiesDetectionJobPropertiesList\": [\r\n",
      "        {\r\n",
      "            \"JobId\": \"1419b6498a3de8563a8ef6a31cdcc327\",\r\n",
      "            \"JobArn\": \"arn:aws:comprehend:us-east-1:139228718159:entities-detection-job/1419b6498a3de8563a8ef6a31cdcc327\",\r\n",
      "            \"JobName\": \"entity_detection_test\",\r\n",
      "            \"JobStatus\": \"IN_PROGRESS\",\r\n",
      "            \"SubmitTime\": 1681856340.227,\r\n",
      "            \"InputDataConfig\": {\r\n",
      "                \"S3Uri\": \"s3://myawsbucket-groupproject/TestText/Entity_Recog_test.txt\",\r\n",
      "                \"InputFormat\": \"ONE_DOC_PER_LINE\"\r\n",
      "            },\r\n",
      "            \"OutputDataConfig\": {\r\n",
      "                \"S3Uri\": \"s3://myawsbucket-groupproject/139228718159-NER-1419b6498a3de8563a8ef6a31cdcc327/output/output.tar.gz\"\r\n",
      "            },\r\n",
      "            \"LanguageCode\": \"en\",\r\n",
      "            \"DataAccessRoleArn\": \"arn:aws:iam::139228718159:role/LabRole\"\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws comprehend list-entities-detection-jobs \\\n",
    "--filter \"JobName = entity_detection_test\" \\\n",
    "--region us-east-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9de832",
   "metadata": {},
   "source": [
    "After the JobStatus becomes to \"COMPLETED\", you can download the output to your SageMaker using the following code:\n",
    "!aws s3 cp \"OUTPUT_DATA_DIRECTORY\" \"NAME_OF_THE_FILE\"\n",
    "You should replace \"OUTPUT_DATA_DIRECTORY\" by the S3 URL of OutputDataConfig in the previous output, and replace \"NAME_OF_THE_FILE\" by the user-defined name of the output data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://myawsbucket-groupproject/139228718159-NER-1419b6498a3de8563a8ef6a31cdcc327/output/output.tar.gz entity_output.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383322f7",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "This service returns a sentiment score of the inputted text, including positive, negative, neutral, and mixed. Each of the sentiment score is assigned with a \"confidence score\", providing an estimate by Amazon Comprehend for that sentiment being dominant. \n",
    "\n",
    "To get the sentiment of the text, run the following code. The detect-sentiment takes three arguments: region, language-code, and the input text. The output will provide you will the general sentiment and the confidence score of each sentiment. For instance, the service labels the sentence, \"We regret to inform that we are unable to invite you to our program this year.\", as negative with 92% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5caa28a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"Sentiment\": \"NEGATIVE\",\r\n",
      "    \"SentimentScore\": {\r\n",
      "        \"Positive\": 0.0031476884614676237,\r\n",
      "        \"Negative\": 0.9206318855285645,\r\n",
      "        \"Neutral\": 0.07557003200054169,\r\n",
      "        \"Mixed\": 0.0006503906333819032\r\n",
      "    }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws comprehend detect-sentiment \\\n",
    "    --region us-east-1 \\\n",
    "    --language-code \"en\" \\\n",
    "    --text \"We regret to inform that we are unable to invite you to our program this year.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20caa61",
   "metadata": {},
   "source": [
    "### Targeted Sentiment Analysis\n",
    "Targeted sentiment analysis provides a more detailed understanding of the sentiments associated with specific entities, such as brands or products, mentioned in your input documents. This approach differs from standard sentiment analysis in that it analyzes the sentiment at the entity level rather than the document level.\n",
    "\n",
    "With targeted sentiment analysis, you can gain insights into the sentiment of specific products or services and identify those that are receiving positive or negative feedback. For example, if you were analyzing a set of restaurant reviews, targeted sentiment analysis could tell you the sentiment associated with particular menu items, like the \"tacos,\" and the behavior of the \"staff.\"\n",
    "\n",
    "In contrast to standard sentiment analysis, which determines the overall sentiment of each input document, targeted sentiment analysis determines the sentiment for entities and attributes mentioned in each document. The output of targeted sentiment analysis includes the identity of the entities mentioned in the documents, the entity type classification, and the sentiment and sentiment score for each entity mention. Additionally, targeted sentiment analysis groups together mentions that correspond to a single entity, known as co-reference groups, to provide a more complete picture of the sentiment associated with that entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5001752",
   "metadata": {},
   "source": [
    "To access the targeted sentiment analysis, run the folliwng code, which takes the same arguments as sentiment analysis: region, language-code, text. The output returns the identified entities and the sentiment score of each entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6c8553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"Entities\": [\r\n",
      "        {\r\n",
      "            \"DescriptiveMentionIndex\": [\r\n",
      "                2\r\n",
      "            ],\r\n",
      "            \"Mentions\": [\r\n",
      "                {\r\n",
      "                    \"Score\": 0.999966025352478,\r\n",
      "                    \"GroupScore\": 1.0,\r\n",
      "                    \"Text\": \"We\",\r\n",
      "                    \"Type\": \"ORGANIZATION\",\r\n",
      "                    \"MentionSentiment\": {\r\n",
      "                        \"Sentiment\": \"NEUTRAL\",\r\n",
      "                        \"SentimentScore\": {\r\n",
      "                            \"Positive\": 0.0,\r\n",
      "                            \"Negative\": 0.0,\r\n",
      "                            \"Neutral\": 1.0,\r\n",
      "                            \"Mixed\": 0.0\r\n",
      "                        }\r\n",
      "                    },\r\n",
      "                    \"BeginOffset\": 0,\r\n",
      "                    \"EndOffset\": 2\r\n",
      "                },\r\n",
      "                {\r\n",
      "                    \"Score\": 0.9999939799308777,\r\n",
      "                    \"GroupScore\": 0.9992619752883911,\r\n",
      "                    \"Text\": \"we\",\r\n",
      "                    \"Type\": \"ORGANIZATION\",\r\n",
      "                    \"MentionSentiment\": {\r\n",
      "                        \"Sentiment\": \"NEUTRAL\",\r\n",
      "                        \"SentimentScore\": {\r\n",
      "                            \"Positive\": 0.0,\r\n",
      "                            \"Negative\": 0.0,\r\n",
      "                            \"Neutral\": 1.0,\r\n",
      "                            \"Mixed\": 0.0\r\n",
      "                        }\r\n",
      "                    },\r\n",
      "                    \"BeginOffset\": 25,\r\n",
      "                    \"EndOffset\": 27\r\n",
      "                },\r\n",
      "                {\r\n",
      "                    \"Score\": 0.9999960064888,\r\n",
      "                    \"GroupScore\": 0.5255290269851685,\r\n",
      "                    \"Text\": \"our\",\r\n",
      "                    \"Type\": \"ORGANIZATION\",\r\n",
      "                    \"MentionSentiment\": {\r\n",
      "                        \"Sentiment\": \"NEUTRAL\",\r\n",
      "                        \"SentimentScore\": {\r\n",
      "                            \"Positive\": 0.0,\r\n",
      "                            \"Negative\": 0.0,\r\n",
      "                            \"Neutral\": 1.0,\r\n",
      "                            \"Mixed\": 0.0\r\n",
      "                        }\r\n",
      "                    },\r\n",
      "                    \"BeginOffset\": 56,\r\n",
      "                    \"EndOffset\": 59\r\n",
      "                }\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"DescriptiveMentionIndex\": [\r\n",
      "                0\r\n",
      "            ],\r\n",
      "            \"Mentions\": [\r\n",
      "                {\r\n",
      "                    \"Score\": 0.9973430037498474,\r\n",
      "                    \"GroupScore\": 1.0,\r\n",
      "                    \"Text\": \"you\",\r\n",
      "                    \"Type\": \"PERSON\",\r\n",
      "                    \"MentionSentiment\": {\r\n",
      "                        \"Sentiment\": \"NEUTRAL\",\r\n",
      "                        \"SentimentScore\": {\r\n",
      "                            \"Positive\": 0.0,\r\n",
      "                            \"Negative\": 0.0,\r\n",
      "                            \"Neutral\": 1.0,\r\n",
      "                            \"Mixed\": 0.0\r\n",
      "                        }\r\n",
      "                    },\r\n",
      "                    \"BeginOffset\": 49,\r\n",
      "                    \"EndOffset\": 52\r\n",
      "                }\r\n",
      "            ]\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"DescriptiveMentionIndex\": [\r\n",
      "                0\r\n",
      "            ],\r\n",
      "            \"Mentions\": [\r\n",
      "                {\r\n",
      "                    \"Score\": 0.9999639987945557,\r\n",
      "                    \"GroupScore\": 1.0,\r\n",
      "                    \"Text\": \"this year\",\r\n",
      "                    \"Type\": \"DATE\",\r\n",
      "                    \"MentionSentiment\": {\r\n",
      "                        \"Sentiment\": \"NEUTRAL\",\r\n",
      "                        \"SentimentScore\": {\r\n",
      "                            \"Positive\": 1.9999999949504854e-06,\r\n",
      "                            \"Negative\": 3.999999989900971e-06,\r\n",
      "                            \"Neutral\": 0.9999920129776001,\r\n",
      "                            \"Mixed\": 1.9999999949504854e-06\r\n",
      "                        }\r\n",
      "                    },\r\n",
      "                    \"BeginOffset\": 68,\r\n",
      "                    \"EndOffset\": 77\r\n",
      "                }\r\n",
      "            ]\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws comprehend detect-targeted-sentiment \\\n",
    "    --region us-east-1 \\\n",
    "    --language-code \"en\" \\\n",
    "    --text \"We regret to inform that we are unable to invite you to our program this year.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
